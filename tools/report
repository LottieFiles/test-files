#!/usr/bin/env python3

import re
import sys
import json
import argparse
import multiprocessing
from pathlib import Path
from PIL import Image, ImageChops
import cv2
import numpy
import sewar.full_ref

from script_utils import DirectoryScanner


class Example:
    def __init__(self, name):
        self.name = name
        self.files = {}
        self.features = set()

    def append(self, file):
        self.files[file.stem] = file

    def mark_features(self, score, features, index, count):
        if len(self.features) == 0:
            self.features = {self.name}

        for feat in self.features:
            if feat not in features:
                features[feat] = {"max": 1, "score": [0] * count}
            elif index == 0:
                features[feat]["max"] += 1

            features[feat]["score"][index] += score


class ExampleFile:
    def __init__(self, path, data):
        self.path = path
        self.stem = path.stem
        self.data = data


def collect_renders(file: Path, relpath: Path, out: dict):
    if file.name == "meta.json" or "asset" in file.name:
        return
    elif file.suffix == ".png":
        basename = re.sub("-[0-9]+$", "", file.stem)
        example_name = str(relpath.parent / basename)
        if example_name not in out:
            out[example_name] = Example(example_name)

        data = Image.open(file).convert("RGBA")
        out[example_name].append(ExampleFile(file, data))


def collect_meta(file: Path, relpath: Path, out: dict):
    if file.name.endswith("-meta.json"):
        example_name = str(relpath / file.stem[:-5])
        if example_name not in out:
            out[example_name] = Example(example_name)

        with open(file) as f:
            features = set()
            for feat in json.load(f).get("features", []):
                split = feat.split("/")
                for i in range(len(split)):
                    features.add("/".join(split[0:i+1]))

        out[example_name].features = features


def pil_to_cv(image: Image.Image):
    transp = numpy.array([0, 0, 0, 0])
    arr = numpy.array(image)
    for y in range(len(arr)):
        for x in range(len(arr[y])):
            if arr[y][x][3] == 0:
                arr[y][x] = transp
    return cv2.cvtColor(arr, cv2.COLOR_RGBA2BGRA)


def convert_img_to_hist(image):
    hist = cv2.calcHist(
        [image], [0, 1, 2], None, [256, 256, 256],
        [0, 256, 0, 256, 0, 256]
    )
    hist = cv2.normalize(hist, hist).flatten()
    return hist


def opencv_similarity(rendered: Image.Image, source: Image.Image):
    cv_source = pil_to_cv(source)
    cv_rendered = pil_to_cv(rendered)

    # orig_hist = convert_img_to_hist(cv_source)
    # rendered_hist = convert_img_to_hist(cv_rendered)
    # cv2.compareHist(orig_hist, rendered_hist, cv2.HISTCMP_CHISQR)

    return sewar.full_ref.uqi(cv_source, cv_rendered)


class PathSettings:
    def __init__(self, prefix: str, relative: Path):
        self.prefix = prefix
        self.relative = relative

    def path(self, path: Path):
        if self.relative:
            path = path.relative_to(self.relative)
        return self.prefix + str(path)


def compare_datum(test_item, reference_item, result, path_settings: PathSettings):
    if test_item is None:
        result["status"] = "missing-data"
        return 0

    result["file"] = path_settings.path(test_item.path)
    test_img = test_item.data
    ref_img = reference_item.data

    if test_img.width != ref_img.width or test_img.height != ref_img.height:
        result["status"] = "size-mismatch"
        return 0

    result["status"] = "ok"
    # hist = ImageChops.difference(test_img, ref_img).histogram()
    # result["histogram"] = hist
    result["ok"] = True
    uqi = opencv_similarity(test_img, ref_img)
    result["uqi"] = uqi
    score = uqi ** 3
    if uqi > 0.95:
        score = 1
    result["score"] = score
    return score


class Job:
    def __init__(self, index: int, path_settings: PathSettings, results_key: str, test_file, reference_file, reference_item):
        self.index = index
        self.result = {
            "file": None,
            "status": "unknown",
            "ok": False,
            "score": 0,
            "uqi": 0,
        }
        self.score = 0
        self.path_settings = path_settings
        self.reference_file = reference_file
        self.test_file = test_file
        self.results_key = results_key
        self.reference_item = reference_item


def process_job(job: Job):
    job.score = compare_datum(job.test_file, job.reference_file, job.result, job.path_settings)
    return job


def compare_data(render_sets, reference_data, path_settings: PathSettings):
    keys = set(reference_data.keys())

    skipped_sets = []
    for render_set in render_sets:
        skipped_sets.append(set(render_set.meta.get("skipped", [])))
        keys |= set(render_set.data.keys())

    skipped = skipped_sets[0]
    for skipped_set in skipped_sets[1:]:
        skipped &= skipped_set

    keys -= skipped

    jobs = []
    for index, render_set in enumerate(render_sets):
        for key in sorted(keys):
            test_item = render_set.data.get(key)

            reference_item = reference_data.get(key)
            if reference_item is None:
                continue

            for name, file in reference_item.files.items():
                results_key = str(file.path)
                jobs.append(Job(
                    index, path_settings, results_key, test_item.files.get(name) if test_item else None, file, reference_item
                ))

    with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:
        jobs = pool.map(process_job, jobs)

    features = {}
    results = {}
    for key in sorted(keys):
        reference_item = reference_data.get(key)
        if reference_item is None:
            continue
        for name, file in reference_item.files.items():
            id = str(Path(key).parent / file.path.name)
            results_key = str(file.path)
            results[results_key] = {
                "id": id,
                "test": key,
                "file": file.path.name,
                "reference": path_settings.path(file.path),
                "score": 0,
                "max": 0,
                "results": [{"time": rs.times.get(id, 0)} for i, rs in enumerate(render_sets)],
            }

    for job in jobs:
        result = results[job.results_key]
        result["score"] += job.score
        result["max"] += 1
        result["results"][job.index].update(job.result)
        job.reference_item.mark_features(job.score, features, job.index, len(render_sets))

    return {
        "meta": [render_set.meta for render_set in render_sets],
        "features": features,
        "tests": sorted(results.values(), key=lambda r: (r["test"], r["file"])),
    }


class RenderSet:
    def __init__(self, path):
        self.data = {}
        DirectoryScanner(collect_renders).scan(path, self.data)
        meta_path = path / "meta.json"
        self.meta = {
            "title": "Lottie",
            "label": "",
            "comment": "",
            "command": "",
            "skipped": [],
            "format": "json",
            "times": {}
        }
        if meta_path.exists:
            with open(meta_path) as f:
                self.meta.update(json.load(f))

        self.times = self.meta.pop("times")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Generate a report based on rendered files"
    )
    parser.add_argument(
        "--metadata",
        help="Root path for the metadata",
        type=Path,
        default=None
    )
    parser.add_argument(
        "--reference",
        help="Root path for the reference data",
        type=Path,
        default=(Path(__file__).parent.parent / "data").relative_to(Path().absolute())
    )
    parser.add_argument(
        "path",
        help="Path of the render set to compare to the reference",
        nargs="+",
        type=Path
    )
    parser.add_argument(
        "--output", "-o",
        help="Output file path",
        type=Path,
        default=None
    )
    parser.add_argument(
        "--prefix",
        help="Prefix for reference file names in the output",
        type=str,
        default=""
    )
    parser.add_argument(
        "--relative",
        help="Make test paths relative to this",
        type=Path,
        default=None
    )

    args = parser.parse_args()

    meta_path = args.metadata or args.reference
    reference = {}
    DirectoryScanner(collect_meta).scan(meta_path, reference)
    DirectoryScanner(collect_renders).scan(args.reference, reference)
    render_sets = [RenderSet(path) for path in args.path]

    data = compare_data(render_sets, reference, PathSettings(args.prefix, args.relative))
    out = sys.stdout if args.output is None else open(args.output, "w")
    json.dump(data, out, indent=4)
    out.write("\n")
